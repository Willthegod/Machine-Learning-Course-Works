{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "import collections\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from regressors import stats\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import *--+\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabaz_score\n",
    "from scipy.spatial import distance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>\n",
    "(a) Download the Anuran Calls (MFCCs) Data Set from: https://archive.ics. uci.edu/ml/datasets/Anuran+Calls+%28MFCCs%29. Choose 70% of the data randomly as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MFCCs_ 1</th>\n",
       "      <th>MFCCs_ 2</th>\n",
       "      <th>MFCCs_ 3</th>\n",
       "      <th>MFCCs_ 4</th>\n",
       "      <th>MFCCs_ 5</th>\n",
       "      <th>MFCCs_ 6</th>\n",
       "      <th>MFCCs_ 7</th>\n",
       "      <th>MFCCs_ 8</th>\n",
       "      <th>MFCCs_ 9</th>\n",
       "      <th>MFCCs_10</th>\n",
       "      <th>...</th>\n",
       "      <th>MFCCs_13</th>\n",
       "      <th>MFCCs_14</th>\n",
       "      <th>MFCCs_15</th>\n",
       "      <th>MFCCs_16</th>\n",
       "      <th>MFCCs_17</th>\n",
       "      <th>MFCCs_18</th>\n",
       "      <th>MFCCs_19</th>\n",
       "      <th>MFCCs_20</th>\n",
       "      <th>MFCCs_21</th>\n",
       "      <th>MFCCs_22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2210</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142665</td>\n",
       "      <td>0.182539</td>\n",
       "      <td>0.466349</td>\n",
       "      <td>0.088505</td>\n",
       "      <td>-0.011861</td>\n",
       "      <td>-0.066383</td>\n",
       "      <td>0.105979</td>\n",
       "      <td>0.305904</td>\n",
       "      <td>0.048669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245476</td>\n",
       "      <td>-0.164588</td>\n",
       "      <td>-0.180686</td>\n",
       "      <td>0.125307</td>\n",
       "      <td>0.111628</td>\n",
       "      <td>-0.017731</td>\n",
       "      <td>-0.050419</td>\n",
       "      <td>-0.067309</td>\n",
       "      <td>0.057707</td>\n",
       "      <td>0.121706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.299430</td>\n",
       "      <td>0.205311</td>\n",
       "      <td>0.590564</td>\n",
       "      <td>0.174769</td>\n",
       "      <td>0.013744</td>\n",
       "      <td>-0.139228</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.273798</td>\n",
       "      <td>0.092782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438382</td>\n",
       "      <td>0.022074</td>\n",
       "      <td>-0.261175</td>\n",
       "      <td>0.020437</td>\n",
       "      <td>0.197870</td>\n",
       "      <td>0.042685</td>\n",
       "      <td>-0.099221</td>\n",
       "      <td>-0.134869</td>\n",
       "      <td>-0.023347</td>\n",
       "      <td>0.146671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.183875</td>\n",
       "      <td>-0.004147</td>\n",
       "      <td>0.136533</td>\n",
       "      <td>0.105952</td>\n",
       "      <td>0.148560</td>\n",
       "      <td>0.146741</td>\n",
       "      <td>-0.022179</td>\n",
       "      <td>-0.120005</td>\n",
       "      <td>0.006283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004177</td>\n",
       "      <td>0.085480</td>\n",
       "      <td>-0.009452</td>\n",
       "      <td>-0.097577</td>\n",
       "      <td>0.008150</td>\n",
       "      <td>0.058082</td>\n",
       "      <td>-0.022958</td>\n",
       "      <td>-0.056031</td>\n",
       "      <td>-0.013966</td>\n",
       "      <td>-0.003480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5010</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.616106</td>\n",
       "      <td>0.703758</td>\n",
       "      <td>0.472784</td>\n",
       "      <td>-0.029115</td>\n",
       "      <td>0.056836</td>\n",
       "      <td>0.006603</td>\n",
       "      <td>0.182854</td>\n",
       "      <td>0.222200</td>\n",
       "      <td>-0.261834</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160116</td>\n",
       "      <td>-0.277389</td>\n",
       "      <td>0.243428</td>\n",
       "      <td>0.092666</td>\n",
       "      <td>-0.153912</td>\n",
       "      <td>-0.033288</td>\n",
       "      <td>0.026084</td>\n",
       "      <td>0.134206</td>\n",
       "      <td>0.082594</td>\n",
       "      <td>-0.161210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4044</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.390029</td>\n",
       "      <td>0.214202</td>\n",
       "      <td>0.598590</td>\n",
       "      <td>0.247795</td>\n",
       "      <td>0.070456</td>\n",
       "      <td>-0.161894</td>\n",
       "      <td>0.069990</td>\n",
       "      <td>0.281743</td>\n",
       "      <td>0.042360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289659</td>\n",
       "      <td>-0.118219</td>\n",
       "      <td>-0.245198</td>\n",
       "      <td>0.065839</td>\n",
       "      <td>0.243514</td>\n",
       "      <td>0.068396</td>\n",
       "      <td>-0.061918</td>\n",
       "      <td>-0.188621</td>\n",
       "      <td>0.017854</td>\n",
       "      <td>0.188410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MFCCs_ 1  MFCCs_ 2  MFCCs_ 3  MFCCs_ 4  MFCCs_ 5  MFCCs_ 6  MFCCs_ 7  \\\n",
       "2210       1.0  0.142665  0.182539  0.466349  0.088505 -0.011861 -0.066383   \n",
       "1792       1.0  0.299430  0.205311  0.590564  0.174769  0.013744 -0.139228   \n",
       "732        1.0  0.183875 -0.004147  0.136533  0.105952  0.148560  0.146741   \n",
       "5010       1.0  0.616106  0.703758  0.472784 -0.029115  0.056836  0.006603   \n",
       "4044       1.0  0.390029  0.214202  0.598590  0.247795  0.070456 -0.161894   \n",
       "\n",
       "      MFCCs_ 8  MFCCs_ 9  MFCCs_10  ...  MFCCs_13  MFCCs_14  MFCCs_15  \\\n",
       "2210  0.105979  0.305904  0.048669  ...  0.245476 -0.164588 -0.180686   \n",
       "1792  0.003150  0.273798  0.092782  ...  0.438382  0.022074 -0.261175   \n",
       "732  -0.022179 -0.120005  0.006283  ... -0.004177  0.085480 -0.009452   \n",
       "5010  0.182854  0.222200 -0.261834  ... -0.160116 -0.277389  0.243428   \n",
       "4044  0.069990  0.281743  0.042360  ...  0.289659 -0.118219 -0.245198   \n",
       "\n",
       "      MFCCs_16  MFCCs_17  MFCCs_18  MFCCs_19  MFCCs_20  MFCCs_21  MFCCs_22  \n",
       "2210  0.125307  0.111628 -0.017731 -0.050419 -0.067309  0.057707  0.121706  \n",
       "1792  0.020437  0.197870  0.042685 -0.099221 -0.134869 -0.023347  0.146671  \n",
       "732  -0.097577  0.008150  0.058082 -0.022958 -0.056031 -0.013966 -0.003480  \n",
       "5010  0.092666 -0.153912 -0.033288  0.026084  0.134206  0.082594 -0.161210  \n",
       "4044  0.065839  0.243514  0.068396 -0.061918 -0.188621  0.017854  0.188410  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Frogs_MFCCs.csv')\n",
    "\n",
    "train_df, test_df = train_test_split(df, train_size=0.7, test_size = 0.3)\n",
    "\n",
    "X_train = train_df.iloc[:,:-4]\n",
    "y_train = train_df.iloc[:,-4:-1]\n",
    "X_test = test_df.iloc[:,:-4]\n",
    "y_test = test_df.iloc[:,-4:-1]\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MFCCs_ 1    5036\n",
       "MFCCs_ 2    5036\n",
       "MFCCs_ 3    5036\n",
       "MFCCs_ 4    5036\n",
       "MFCCs_ 5    5036\n",
       "MFCCs_ 6    5036\n",
       "MFCCs_ 7    5036\n",
       "MFCCs_ 8    5036\n",
       "MFCCs_ 9    5036\n",
       "MFCCs_10    5036\n",
       "MFCCs_11    5036\n",
       "MFCCs_12    5036\n",
       "MFCCs_13    5036\n",
       "MFCCs_14    5036\n",
       "MFCCs_15    5036\n",
       "MFCCs_16    5036\n",
       "MFCCs_17    5036\n",
       "MFCCs_18    5036\n",
       "MFCCs_19    5036\n",
       "MFCCs_20    5036\n",
       "MFCCs_21    5036\n",
       "MFCCs_22    5036\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MFCCs_ 1    2159\n",
       "MFCCs_ 2    2159\n",
       "MFCCs_ 3    2159\n",
       "MFCCs_ 4    2159\n",
       "MFCCs_ 5    2159\n",
       "MFCCs_ 6    2159\n",
       "MFCCs_ 7    2159\n",
       "MFCCs_ 8    2159\n",
       "MFCCs_ 9    2159\n",
       "MFCCs_10    2159\n",
       "MFCCs_11    2159\n",
       "MFCCs_12    2159\n",
       "MFCCs_13    2159\n",
       "MFCCs_14    2159\n",
       "MFCCs_15    2159\n",
       "MFCCs_16    2159\n",
       "MFCCs_17    2159\n",
       "MFCCs_18    2159\n",
       "MFCCs_19    2159\n",
       "MFCCs_20    2159\n",
       "MFCCs_21    2159\n",
       "MFCCs_22    2159\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2159/7195=0.300069, \n",
      " about 30 percent of the data is in the Test set and the 70 percent is in the Training set\n"
     ]
    }
   ],
   "source": [
    "print('2159/7195=%f, \\n about 30 percent of the data is in the Test set and the 70 percent is in the Training set'%(2159/7195))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5> (b) Each instance has three labels: Families, Genus, and Species. Each of the labels has multiple classes. We wish to solve a multi-class and multi-label problem. One of the most important approaches to multi-class classification is to train a classifier for each label. We first try this approach:\n",
    "\n",
    "i. Research exact match and hamming score/ loss methods for evaluating multi-label classification and use them in evaluating the classifiers in this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact match (also called Subset accuracy): is the most strict metric, indicating the percentage of samples that have all their labels classified correctly. The disadvantage of this measure is that multi-class classification problems have a chance of being partially correct, but here we ignore those partially correct matches.\n",
    "\n",
    "## Hamming-Loss (Example based measure): In simplest of terms, Hamming-Loss is the fraction of labels that are incorrectly predicted, i.e., the fraction of the wrong labels to the total number of labels. The method choose in hamming loss was to give each label equal weight.\n",
    "\n",
    "## In general, there is no magical metric that is the best for every problem. In every problem you have different needs, and you should optimize for them.\n",
    "\n",
    "sources: https://stats.stackexchange.com/questions/336820/what-is-a-hamming-loss-will-we-consider-it-for-an-imbalanced-binary-classifier https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size= 5> ii. Train a SVM for each of the labels, using Gaussian kernels and one versus all classifiers. Determine the weight of the SVM penalty and the width of the Gaussian Kernel using 10 fold cross validation.1 You are welcome to try to solve the problem with both standardized and raw attributes and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: C = 1111.112000, Gamma = 1.333889\n"
     ]
    }
   ],
   "source": [
    "# family \n",
    "y_train_family = y_train['Family']\n",
    "Cs = np.linspace(0.001,10000,10)\n",
    "gammas = np.linspace(0.001,3,10)\n",
    "param_set = {'C': Cs, 'gamma' : gammas}\n",
    "grid_search_cv = GridSearchCV(SVC(kernel='rbf'), param_set , cv=10)\n",
    "grid_search_cv.fit(X_train, y_train_family)\n",
    "best_param = grid_search_cv.best_params_\n",
    "print('Best Parameters: C = %f, Gamma = %f'%(best_param['C'],best_param['gamma']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9898100972672533\n",
      "Hamming Loss: 0.010189902732746642\n"
     ]
    }
   ],
   "source": [
    "pred = pd.DataFrame()\n",
    "pred['Family_1'] = grid_search_cv.predict(X_test)\n",
    "\n",
    "acc_score = accuracy_score(y_test['Family'], pred['Family_1'])\n",
    "hamming_l = hamming_loss(y_test['Family'], pred['Family_1'])\n",
    "print('Accuracy Score:', acc_score)\n",
    "print('Hamming Loss:', hamming_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: C = 1111.112000, Gamma = 1.333889\n"
     ]
    }
   ],
   "source": [
    "# Genus\n",
    "y_train_genus = y_train['Genus']\n",
    "Cs = np.linspace(0.001,10000,10)\n",
    "gammas = np.linspace(0.001,3,10)\n",
    "param_set = {'C': Cs, 'gamma' : gammas}\n",
    "grid_search_cv = GridSearchCV(SVC(kernel='rbf'), param_set , cv=10)\n",
    "grid_search_cv.fit(X_train, y_train_genus)\n",
    "best_param = grid_search_cv.best_params_\n",
    "print('Best Parameters: C = %f, Gamma = %f'%(best_param['C'],best_param['gamma']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9884205650764243\n",
      "Hamming Loss: 0.01157943492357573\n"
     ]
    }
   ],
   "source": [
    "pred['Genus_1'] = grid_search_cv.predict(X_test)\n",
    "\n",
    "acc_score = accuracy_score(y_test['Genus'], pred['Genus_1'])\n",
    "hamming_l = hamming_loss(y_test['Genus'], pred['Genus_1'])\n",
    "print('Accuracy Score:', acc_score)\n",
    "print('Hamming Loss:', hamming_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: C = 1111.112000, Gamma = 2.000333\n"
     ]
    }
   ],
   "source": [
    "# Species\n",
    "y_train_species = y_train['Species']\n",
    "Cs = np.linspace(0.001,10000,10)\n",
    "gammas = np.linspace(0.001,3,10)\n",
    "param_set = {'C': Cs, 'gamma' : gammas}\n",
    "grid_search_cv = GridSearchCV(SVC(kernel='rbf'), param_set , cv=10)\n",
    "grid_search_cv.fit(X_train, y_train_species)\n",
    "best_param = grid_search_cv.best_params_\n",
    "print('Best Parameters: C = %f, Gamma = %f'%(best_param['C'],best_param['gamma']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9888837424733673\n",
      "Hamming Loss: 0.0111162575266327\n"
     ]
    }
   ],
   "source": [
    "pred['Species_1'] = grid_search_cv.predict(X_test)\n",
    "\n",
    "acc_score = accuracy_score(y_test['Species'], pred['Species_1'])\n",
    "hamming_l = hamming_loss(y_test['Species'], pred['Species_1'])\n",
    "print('Accuracy Score:', acc_score)\n",
    "print('Hamming Loss:', hamming_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>iii. Repeat 1(b)ii with L1-penalized SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: C = 166.810054, Best Score = 0.935266\n"
     ]
    }
   ],
   "source": [
    "# family \n",
    "\n",
    "Cs = np.logspace(-5, 8, 10)\n",
    "param_set = {'C': Cs}\n",
    "l1_svm = LinearSVC(penalty= 'l1', dual=False, max_iter = 5000)\n",
    "grid_search_cv = GridSearchCV(l1_svm, param_set , cv=10, scoring = 'accuracy') \n",
    "grid_search_cv.fit(scaled_X_train, y_train_family)\n",
    "best_param = grid_search_cv.best_params_\n",
    "best_score = grid_search_cv.best_score_\n",
    "print('Best Parameters: C = %f, Best Score = %f'%(best_param['C'],best_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9300602130616026\n",
      "Hamming Loss: 0.0699397869383974\n"
     ]
    }
   ],
   "source": [
    "pred_l1 = pd.DataFrame()\n",
    "pred_l1['Family_1'] = grid_search_cv.predict(scaled_X_test)\n",
    "\n",
    "acc_score = accuracy_score(y_test['Family'], pred_l1['Family_1'])\n",
    "hamming_l = hamming_loss(y_test['Family'], pred_l1['Family_1'])\n",
    "print('Accuracy Score:', acc_score)\n",
    "print('Hamming Loss:', hamming_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: C = 166.810054, Best Score = 0.957109\n"
     ]
    }
   ],
   "source": [
    "# Genus\n",
    "\n",
    "Cs = np.logspace(-5, 8, 10)\n",
    "param_set = {'C': Cs}\n",
    "l1_svm = LinearSVC(penalty= 'l1', dual=False, max_iter = 5000)\n",
    "grid_search_cv = GridSearchCV(l1_svm, param_set , cv=10, scoring = 'accuracy') \n",
    "grid_search_cv.fit(scaled_X_train, y_train_genus)\n",
    "best_param = grid_search_cv.best_params_\n",
    "best_score = grid_search_cv.best_score_\n",
    "print('Best Parameters: C = %f, Best Score = %f'%(best_param['C'],best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9448818897637795\n",
      "Hamming Loss: 0.05511811023622047\n"
     ]
    }
   ],
   "source": [
    "pred_l1['Genus_1'] = grid_search_cv.predict(scaled_X_test)\n",
    "\n",
    "acc_score = accuracy_score(y_test['Genus'], pred_l1['Genus_1'])\n",
    "hamming_l = hamming_loss(y_test['Genus'], pred_l1['Genus_1'])\n",
    "print('Accuracy Score:', acc_score)\n",
    "print('Hamming Loss:', hamming_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: C = 5.994843, Best Score = 0.962272\n"
     ]
    }
   ],
   "source": [
    "# Species\n",
    "\n",
    "Cs = np.logspace(-5, 8, 10)\n",
    "param_set = {'C': Cs}\n",
    "l1_svm = LinearSVC(penalty= 'l1', dual=False, max_iter = 5000)\n",
    "grid_search_cv = GridSearchCV(l1_svm, param_set , cv=10, scoring = 'accuracy') \n",
    "grid_search_cv.fit(scaled_X_train, y_train_species)\n",
    "best_param = grid_search_cv.best_params_\n",
    "best_score = grid_search_cv.best_score_\n",
    "print('Best Parameters: C = %f, Best Score = %f'%(best_param['C'],best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9490504863362668\n",
      "Hamming Loss: 0.05094951366373321\n"
     ]
    }
   ],
   "source": [
    "pred_l1['Species_1'] = grid_search_cv.predict(scaled_X_test)\n",
    "\n",
    "acc_score = accuracy_score(y_test['Species'], pred_l1['Species_1'])\n",
    "hamming_l = hamming_loss(y_test['Species'], pred_l1['Species_1'])\n",
    "print('Accuracy Score:', acc_score)\n",
    "print('Hamming Loss:', hamming_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5> iv. Repeat 1(b)iii by using SMOTE or any other method you know to remedy class imbalanceÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: C = 166.810054, Best Score = 0.953050\n"
     ]
    }
   ],
   "source": [
    "# Family \n",
    "\n",
    "sm = SMOTE()\n",
    "pred_smote = pd.DataFrame()\n",
    "\n",
    "X_res, y_res = sm.fit_sample(scaled_X_train, y_train['Family'])\n",
    "X_res_family = pd.DataFrame(X_res)\n",
    "Y_res_family = pd.DataFrame(y_res)\n",
    "\n",
    "Cs = np.logspace(-5, 8, 10)\n",
    "param_set = {'C': Cs}\n",
    "l1_svm = LinearSVC(penalty= 'l1', dual=False, max_iter = 5000)\n",
    "grid_search_cv = GridSearchCV(l1_svm, param_set , cv=10, scoring = 'accuracy') \n",
    "grid_search_cv.fit(X_res_family, Y_res_family)\n",
    "best_param = grid_search_cv.best_params_\n",
    "best_score = grid_search_cv.best_score_\n",
    "print('Best Parameters: C = %f, Best Score = %f'%(best_param['C'],best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9152385363594256\n",
      "Hamming Loss: 0.08476146364057434\n"
     ]
    }
   ],
   "source": [
    "pred_smote['Family_1'] = grid_search_cv.predict(scaled_X_test)\n",
    "\n",
    "acc_score = accuracy_score(y_test['Family'], pred_smote['Family_1'])\n",
    "hamming_l = hamming_loss(y_test['Family'], pred_smote['Family_1'])\n",
    "print('Accuracy Score:', acc_score)\n",
    "print('Hamming Loss:', hamming_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: C = 166.810054, Best Score = 0.960441\n"
     ]
    }
   ],
   "source": [
    "# Genus \n",
    "\n",
    "X_res, y_res = sm.fit_sample(scaled_X_train, y_train['Genus'])\n",
    "X_res_genus  = pd.DataFrame(X_res)\n",
    "Y_res_genus  = pd.DataFrame(y_res)\n",
    "\n",
    "Cs = np.logspace(-5, 8, 10)\n",
    "param_set = {'C': Cs}\n",
    "l1_svm = LinearSVC(penalty= 'l1', dual=False, max_iter = 5000)\n",
    "grid_search_cv = GridSearchCV(l1_svm, param_set , cv=10, scoring = 'accuracy') \n",
    "grid_search_cv.fit(X_res_genus, Y_res_genus)\n",
    "best_param = grid_search_cv.best_params_\n",
    "best_score = grid_search_cv.best_score_\n",
    "print('Best Parameters: C = %f, Best Score = %f'%(best_param['C'],best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.905048633626679\n",
      "Hamming Loss: 0.09495136637332098\n"
     ]
    }
   ],
   "source": [
    "pred_smote['Genus_1'] = grid_search_cv.predict(scaled_X_test)\n",
    "\n",
    "acc_score = accuracy_score(y_test['Genus'], pred_smote['Genus_1'])\n",
    "hamming_l = hamming_loss(y_test['Genus'], pred_smote['Genus_1'])\n",
    "print('Accuracy Score:', acc_score)\n",
    "print('Hamming Loss:', hamming_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: C = 166.810054, Best Score = 0.963792\n"
     ]
    }
   ],
   "source": [
    "# Species \n",
    "\n",
    "X_res, y_res = sm.fit_sample(scaled_X_train, y_train['Species'])\n",
    "X_res_species = pd.DataFrame(X_res)\n",
    "Y_res_species = pd.DataFrame(y_res)\n",
    "\n",
    "Cs = np.logspace(-5, 8, 10)\n",
    "param_set = {'C': Cs}\n",
    "l1_svm = LinearSVC(penalty= 'l1', dual=False, max_iter = 5000)\n",
    "grid_search_cv = GridSearchCV(l1_svm, param_set , cv=10, scoring = 'accuracy') \n",
    "grid_search_cv.fit(X_res_species, Y_res_species)\n",
    "best_param = grid_search_cv.best_params_\n",
    "best_score = grid_search_cv.best_score_\n",
    "print('Best Parameters: C = %f, Best Score = %f'%(best_param['C'],best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9485873089393237\n",
      "Hamming Loss: 0.05141269106067624\n"
     ]
    }
   ],
   "source": [
    "pred_smote['Species_1'] = grid_search_cv.predict(scaled_X_test)\n",
    "\n",
    "acc_score = accuracy_score(y_test['Species'], pred_smote['Species_1'])\n",
    "hamming_l = hamming_loss(y_test['Species'], pred_smote['Species_1'])\n",
    "print('Accuracy Score:', acc_score)\n",
    "print('Hamming Loss:', hamming_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =5 >Report your conclusions about the classifiers you trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)ii Gaussian Kernel SVM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| XXX            | Family               | Genus               | Species            |\n",
    "|----------------|----------------------|---------------------|--------------------|\n",
    "| Best C         | 1111.112000          | 1111.112000         | 1111.112000        |\n",
    "| Best Gamma     | 1.333889             | 1.333889            | 2.000333           |\n",
    "| Accuracy Score | 0.9898100972672533   | 0.9884205650764243  | 0.9888837424733673 |\n",
    "| Hamming Loss   | 0.010189902732746642 | 0.01157943492357573 | 0.0111162575266327 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)iii L1-penalized SVMs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| XXX            | Family             | Genus               | Species             |\n",
    "|----------------|--------------------|---------------------|---------------------|\n",
    "| Best C         | 166.810054         | 166.810054          | 5.994843            |\n",
    "| Best Score     | 0.935266           | 0.957109            | 0.962272            |\n",
    "| Accuracy Score | 0.9300602130616026 | 0.9448818897637795  | 0.9490504863362668  |\n",
    "| Hamming Loss   | 0.0699397869383974 | 0.05511811023622047 | 0.05094951366373321 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)iv L1-penalized SVMs with SMOTE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| XXX            | Family              | Genus               | Species             |\n",
    "|----------------|---------------------|---------------------|---------------------|\n",
    "| Best C         | 166.810054          | 166.810054          | 166.810054          |\n",
    "| Best Score     | 0.953050            | 0.960441            | 0.963792            |\n",
    "| Accuracy Score | 0.9152385363594256  | 0.905048633626679   | 0.9485873089393237  |\n",
    "| Hamming Loss   | 0.08476146364057434 | 0.09495136637332098 | 0.05141269106067624 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the above tables , C is the weight of the SVM parameter and gamma is the width of the Gaussian Kernel. \n",
    "## The SVM using Gaussian Kernels had the best the Accuracy Score and Hamming Loss. \n",
    "## So, this non-linear SVM performed better than the L1-penalized SVMs. Using SMOTE to remedy class imbalance did not improve the L1-penalized SVM model. Accordint to the Accuracy Scores and Hamming Loss for the three classifiers trained, the dataset is probably not linearly separated and was able to fit better with a non-linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------\n",
    "<font size = 5>2. K-Means Clustering on a Multi-Class and Multi-Label Data Set Monte-Carlo Simulation: Perform the following procedures 50 times, and report\n",
    "the average and standard deviation of the 50 Hamming Distances that you calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>(a) Use k-means clustering on the whole Anuran Calls (MFCCs) Data Set (do not split the data into train and test, as we are not performing supervised learning in this exercise). Choose k âˆˆ {1, 2, . . . , 50} automatically based on one of the methods provided in the slides (CH or Gap Statistics or scree plots or Silhouettes) or any other method you know. \n",
    "\n",
    "(b) In each cluster, determine which family is the majority by reading the true labels. Repeat for genus and species.\n",
    "\n",
    "\n",
    "(c) Now for each cluster you have a majority label triplet (family, genus, species). Calculate the average Hamming distance, Hamming score, and Hamming loss5 between the true labels and the labels assigned by clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_cluster():\n",
    "    \n",
    "    cluster_df = pd.read_csv('Frogs_MFCCs.csv')\n",
    "    X_df = cluster_df.iloc[:,:-4]\n",
    "    k_list = list(range(2,51)) \n",
    "    k_scores_ch = []\n",
    "    for k in k_list: \n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        kmeans.fit(X_df)  \n",
    "        k_labels = kmeans.labels_\n",
    "        CH = calinski_harabaz_score(X_df, k_labels)\n",
    "        k_scores_ch.append(CH)\n",
    "\n",
    "    opt_k_ch = k_list[k_scores_ch.index(max(k_scores_ch))]\n",
    "    kmeans = KMeans(n_clusters=opt_k_ch).fit(X_df)\n",
    "    cluster_df['cluster'] = kmeans.labels_\n",
    "\n",
    "    for c in range(0,opt_k_ch):\n",
    "        family = cluster_df[cluster_df.cluster == c]['Family'].value_counts().index[0]\n",
    "        cluster_df.loc[cluster_df.cluster == c, 'Family_Majority'] = family\n",
    "        genus = cluster_df[cluster_df.cluster == c]['Genus'].value_counts().index[0]\n",
    "        cluster_df.loc[cluster_df.cluster == c, 'Genus_Majority'] = genus\n",
    "        species = cluster_df[cluster_df.cluster == c]['Species'].value_counts().index[0]\n",
    "        cluster_df.loc[cluster_df.cluster == c, 'Species_Majority'] = species\n",
    "    score_list = []\n",
    "    \n",
    "    labels = ['Family', 'Genus', 'Species']\n",
    "\n",
    "    for c in range(0,opt_k_ch):\n",
    "        \n",
    "        cluster_score_list = []\n",
    "        cluster = cluster_df[cluster_df.cluster == c]\n",
    "        for l in labels:\n",
    "            cluster_label = l + '_Majority'\n",
    "\n",
    "            score = hamming_loss(cluster[l], cluster[cluster_label])\n",
    "            cluster_score_list.append(score)\n",
    "            print('Cluster %d >> %s: %f'%(c,l,score))\n",
    "            \n",
    "        avg_cluster_loss = np.mean(cluster_score_list)\n",
    "        score_list.append(avg_cluster_loss)\n",
    "        print('Average for Cluster %d: %f'%(c,avg_cluster_loss))\n",
    "        \n",
    "    average_loss = np.mean(score_list)\n",
    "    print('Average for iteration:',average_loss)\n",
    "    return(average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number -> 1 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 2 : \n",
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 3 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 4 : \n",
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 5 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 6 : \n",
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 7 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 8 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 9 : \n",
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 10 : \n",
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 11 : \n",
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 12 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 13 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 14 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 15 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 16 : \n",
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 17 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 18 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 19 : \n",
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 20 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 21 : \n",
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 22 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 23 : \n",
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 24 : \n",
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 25 : \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 26 : \n",
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 27 : \n",
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 28 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 29 : \n",
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 30 : \n",
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 31 : \n",
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 32 : \n",
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 33 : \n",
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 34 : \n",
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 35 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 36 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 37 : \n",
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 38 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 39 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 40 : \n",
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 41 : \n",
      "Cluster 0 >> Family: 0.432583\n",
      "Cluster 0 >> Genus: 0.561023\n",
      "Cluster 0 >> Species: 0.692244\n",
      "Average for Cluster 0: 0.561950\n",
      "Cluster 1 >> Family: 0.034464\n",
      "Cluster 1 >> Genus: 0.035575\n",
      "Cluster 1 >> Species: 0.035575\n",
      "Average for Cluster 1: 0.035205\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 42 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 43 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 44 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 45 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 46 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 47 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 48 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 49 : \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n",
      "Iteration Number -> 50 : \n",
      "Cluster 0 >> Family: 0.034464\n",
      "Cluster 0 >> Genus: 0.035575\n",
      "Cluster 0 >> Species: 0.035575\n",
      "Average for Cluster 0: 0.035205\n",
      "Cluster 1 >> Family: 0.432583\n",
      "Cluster 1 >> Genus: 0.561023\n",
      "Cluster 1 >> Species: 0.692244\n",
      "Average for Cluster 1: 0.561950\n",
      "Average for iteration: 0.2985772581674484\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "hamming_loss_list = []\n",
    "\n",
    "for i in range(1,51):\n",
    "    \n",
    "    print('Iteration Number -> %d : '%(i))\n",
    "    \n",
    "    hamming_l = kmeans_cluster()\n",
    "    hamming_loss_list.append(hamming_l)\n",
    "    \n",
    "    print('------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best K is 2 selected using CH in above 50 iterations.\n",
      "Average hamming loss in all iterations: 0.2985772581674484\n",
      "STD hamming loss in all iterations: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('the best K is 2 selected using CH in above 50 iterations.')\n",
    "hamming_std = np.std(np.array(hamming_loss_list))\n",
    "hamming_avg = np.mean(np.array(hamming_loss_list))\n",
    "print('Average hamming loss in all iterations:', hamming_avg)\n",
    "print('STD hamming loss in all iterations:', hamming_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ISLR 10.7.2 : Suppose that we have four observations, for which we compute a dissimilarity matrix, given by [] For instance, the dissimilarity between the first and second obser- vations is 0.3, and the dissimilarity between the second and fourth observations is 0.8.\n",
    "(a) On the basis of this dissimilarity matrix, sketch the dendrogram that results from hierarchically clustering these four observa- tions using complete linkage. Be sure to indicate on the plot the height at which each fusion occurs, as well as the observations corresponding to each leaf in the dendrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. We already have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgur.com/oVhhRal.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url=\"https://i.imgur.com/oVhhRal.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. i=4 : We may see that 0.3 is the minimum dissimilarity, so we fuse observations 1 and 2 to form cluster (1,2) at hight 0.3. We now have the new dissimilarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://imgur.com/FcPRAny.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://imgur.com/FcPRAny.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i=3  : We now see that the minimum dissimilarity is 0.45, so we fuse observations 3 and 4 to form cluster (3,4) at height 0.45. We now have the new dissimilarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://imgur.com/VaNpWcb.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://imgur.com/VaNpWcb.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i=4  : It remains to fuse clusters (1,2) and (3,4) to form cluster ((1,2),(3,4)) at height 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://imgur.com/2AJEAD7.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://imgur.com/2AJEAD7.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Repeat (a), this time using single linkage clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgur.com/oVhhRal.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://i.imgur.com/oVhhRal.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: i=4 : We see that 0.3 is the minimum dissimilarity, so we fuse observations 1 and 2 to form cluster (1,2) at hight 0.3. We now have the new dissimilarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://imgur.com/OszUoBX.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://imgur.com/OszUoBX.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i=3  : Now we see that the minimum dissimilarity is 0.4, so we fuse cluster (1,2) and observation 3 to form cluster ((1,2),3) at height 0.4. We now have the new dissimilarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://imgur.com/MyAu9bI.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://imgur.com/MyAu9bI.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i=4 : It remains to fuse clusters ((1,2),3) and observation 4 to form cluster (((1,2),3),4) at height 0.45."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://imgur.com/MqptR5y.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://imgur.com/MqptR5y.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Suppose that we cut the dendogram obtained in (a) such that two clusters result. Which observations are in each cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this case, we have clusters (1,2) and (3,4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Suppose that we cut the dendogram obtained in (b) such that two clusters result. Which observations are in each cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this case, we have clusters ((1,2),3) and (4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) It is mentioned in the chapter that at each fusion in the den- drogram, the position of the two clusters being fused can be swapped without changing the meaning of the dendrogram. Draw a dendrogram that is equivalent to the dendrogram in (a), for which two or more of the leaves are repositioned, but for which the meaning of the dendrogram is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://imgur.com/ty71XHn.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://imgur.com/ty71XHn.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
